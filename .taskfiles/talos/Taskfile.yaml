---
version: '3'

vars:
  SYSTEM_UPGRADE_KS: '{{.KUBERNETES_DIR}}/apps/system-upgrade/system-upgrade-controller/ks.yaml'

env:
  KUBERNETES_VERSION:
    sh: yq '.spec.postBuild.substitute.KUBERNETES_VERSION | select(.)' {{.SYSTEM_UPGRADE_KS}}
  TALOS_VERSION:
    sh: yq '.spec.postBuild.substitute.TALOS_VERSION | select(.)' {{.SYSTEM_UPGRADE_KS}}

tasks:

  list-node-types:
    desc: List all available node types
    dir: talos
    cmds:
      - |
        echo "Available node types:"
        find node-types -maxdepth 1 -type d -not -path node-types | sed 's|node-types/||' | sort
    silent: true

  list-nodes:
    desc: List all nodes and their types
    dir: talos
    cmds:
      - |
        echo "Node mappings:"
        yq '.nodes | to_entries | .[] | .key + " -> " + .value' node-mapping.yaml
    preconditions:
      - test -f {{.TALOS_DIR}}/node-mapping.yaml
      - which yq
    silent: true

  regenerate-schematic:
    desc: Regenerate schematic ID for a node type [NODE_TYPE=required]
    dir: talos
    requires:
      vars: [NODE_TYPE]
    cmds:
      - |
        echo "Regenerating schematic for {{.NODE_TYPE}}..."
        SCHEMATIC_ID=$(curl --silent -X POST --data-binary @node-types/{{.NODE_TYPE}}/schematic.yaml https://factory.talos.dev/schematics | jq -r .id)
        echo "New schematic ID: $SCHEMATIC_ID"
        echo "You may need to update TALOS_SCHEMATIC in your environment"
    preconditions:
      - test -f {{.TALOS_DIR}}/node-types/{{.NODE_TYPE}}/schematic.yaml
      - which talosctl

  add-node-type:
    desc: Add a new node type [NODE_TYPE=required] [TEMPLATE_SOURCE=optional]
    dir: talos
    requires:
      vars: [NODE_TYPE]
    vars:
      TEMPLATE_SOURCE: '{{.TEMPLATE_SOURCE | default "upstream.j2"}}'
    cmds:
      - |
        echo "Creating new node type: {{.NODE_TYPE}}"
        mkdir -p node-types/{{.NODE_TYPE}}/controlplane
        cp {{.TEMPLATE_SOURCE}} node-types/{{.NODE_TYPE}}/template.j2
        cp schematic.yaml node-types/{{.NODE_TYPE}}/schematic.yaml
        echo "Node type {{.NODE_TYPE}} created!"
        echo "Next steps:"
        echo "1. Edit node-types/{{.NODE_TYPE}}/template.j2 for hardware-specific config"
        echo "2. Edit node-types/{{.NODE_TYPE}}/schematic.yaml for required extensions"
        echo "3. Add node IP mappings to node-mapping.yaml"
        echo "4. Put controlplane configs in node-types/{{.NODE_TYPE}}/controlplane/"
    preconditions:
      - test ! -d {{.TALOS_DIR}}/node-types/{{.NODE_TYPE}}
      - test -f {{.TALOS_DIR}}/{{.TEMPLATE_SOURCE}}

  generate-config:
    desc: Generate talosconfig from secrets bundle
    dir: talos
    cmds:
      - talosctl gen config --with-secrets {{.ROOT_DIR}}/secrets.yaml --output-types talosconfig -o talosconfig --force homeops {{.CONTROL_PLANE_ENDPOINT}}
      - talosctl config endpoint {{.TALOS_ENDPOINTS}}
      - talosctl config nodes {{.TALOS_NODES}}
    sources:
      - "{{.ROOT_DIR}}/secrets.yaml"
    generates:
      - talosconfig
    preconditions:
      - sh: test -f {{.ROOT_DIR}}/secrets.yaml
        msg: "secrets.yaml not found. Run 'task talos:build-secrets' to build it from 1Password"
      - which talosctl

  generate-config-healthy:
    desc: Generate talosconfig with only healthy/reachable nodes [NODES=optional]
    dir: talos
    vars:
      HEALTHY_NODES: '{{.NODES | default ""}}'
    cmds:
      - |
        # Generate base config
        talosctl gen config --with-secrets {{.ROOT_DIR}}/secrets.yaml --output-types talosconfig -o talosconfig --force homeops {{.CONTROL_PLANE_ENDPOINT}}
        
        # Set endpoints to all nodes initially for health check
        talosctl config endpoint {{.TALOS_ENDPOINTS}}
        
        # If specific nodes provided, use them; otherwise test all nodes
        if [ -n "{{.HEALTHY_NODES}}" ]; then
          NODES_TO_USE="{{.HEALTHY_NODES}}"
          echo "Using specified nodes: $NODES_TO_USE"
        else
          echo "Testing node connectivity..."
          NODES_TO_USE=""
          # Convert comma-separated string to space-separated for iteration
          ALL_NODES=$(echo "{{.TALOS_NODES}}" | tr ',' ' ')
          for node in $ALL_NODES; do
            echo "Testing $node..."
            if timeout 5 talosctl --nodes "$node" get machineconfig >/dev/null 2>&1; then
              echo "✓ $node is healthy"
              if [ -z "$NODES_TO_USE" ]; then
                NODES_TO_USE="$node"
              else
                NODES_TO_USE="$NODES_TO_USE,$node"
              fi
            else
              echo "✗ $node is unreachable, excluding from config"
            fi
          done
        fi
        
        # Configure with healthy nodes only
        if [ -n "$NODES_TO_USE" ]; then
          echo "Configuring talosconfig with nodes: $NODES_TO_USE"
          talosctl config nodes "$NODES_TO_USE"
          echo "Successfully configured talosconfig with healthy nodes"
        else
          echo "ERROR: No healthy nodes found!"
          exit 1
        fi
    preconditions:
      - sh: test -f {{.ROOT_DIR}}/secrets.yaml
        msg: "secrets.yaml not found. Run 'task talos:build-secrets' to build it from 1Password"
      - which talosctl

  apply-config:
    desc: Apply Talos configuration to a node [NODE=required] [INSECURE=false]
    dir: talos
    requires:
      vars: [NODE]
    vars:
      INSECURE: '{{.INSECURE | default "false"}}'
      HOSTNAME:
        sh: |
          # First check if NODE matches a config file directly (e.g., home01, home02, home03)
          if test -f "static-configs/{{.NODE}}.yaml"; then
            echo "{{.NODE}}"
          else
            # Use node mapping to determine hostname from IP address
            # Extract hostname from node mapping comments
            grep "{{.NODE}}" node-mapping.yaml | grep -o 'home[0-9][0-9]' | head -1 || echo "unknown"
          fi
      NODE_IP:
        sh: |
          # If NODE is already an IP address, use it directly
          if echo "{{.NODE}}" | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$' >/dev/null; then
            echo "{{.NODE}}"
          else
            # Map hostname to IP address using node mapping file
            grep "{{.NODE}}" node-mapping.yaml | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+\.[0-9]\+' | head -1 || echo "{{.NODE}}"
          fi
      CONFIG_FILE: static-configs/{{.HOSTNAME}}.yaml
      INSECURE_FLAG:
        sh: |
          if [ "{{.INSECURE}}" = "true" ]; then
            echo "--insecure"
          else
            echo ""
          fi
    cmds:
      - |
        echo "Using hostname: {{.HOSTNAME}}"
        echo "Using node IP: {{.NODE_IP}}"
        if [ "{{.INSECURE}}" = "true" ]; then
          echo "Using insecure mode (--insecure flag)"
        fi
        echo "Applying {{.CONFIG_FILE}} to node {{.NODE_IP}}"
        cat {{.CONFIG_FILE}} | op inject > /tmp/talos-config-{{.HOSTNAME}}.yaml
        talosctl apply-config --nodes "{{.NODE_IP}}" {{.INSECURE_FLAG}} --file /tmp/talos-config-{{.HOSTNAME}}.yaml
        rm /tmp/talos-config-{{.HOSTNAME}}.yaml
    preconditions:
      - op user get --me
      - test -f {{.TALOS_DIR}}/{{.CONFIG_FILE}}
      - which op talosctl

  upgrade-node:
    desc: Upgrade Talos on a single node [NODE=required]
    cmd: talosctl --nodes {{.NODE}} upgrade --image={{.TALOS_IMAGE}} --timeout=10m
    vars:
      MACHINE_TYPE:
        sh: talosctl --nodes {{.NODE}} get machinetypes --output=jsonpath='{.spec}'
      TALOS_IMAGE:
        sh: |-
          talosctl --nodes {{.NODE}} get machineconfig --output=jsonpath='{.spec}' \
            | yq '.machine.install.image | select(. != null)'
    requires:
      vars: [NODE]
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.NODE}} get machineconfig
      - which minijinja-cli talosctl yq

  upgrade-k8s:
    desc: Upgrade Kubernetes across the whole cluster
    cmd: talosctl --nodes {{.RANDOM_CONTROLLER}} upgrade-k8s --to $KUBERNETES_VERSION
    vars:
      RANDOM_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | sort -R | head -1
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.RANDOM_CONTROLLER}} get machineconfig
      - which jq talosctl

  reboot-node:
    desc: Reboot Talos on a single node [NODE=required] [MODE=default]
    cmd: talosctl --nodes {{.NODE}} reboot --mode={{.MODE}}
    vars:
      MODE: '{{.MODE | default "default"}}'
    requires:
      vars: [NODE]
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.NODE}} get machineconfig
      - which talosctl

  shutdown-cluster:
    desc: Shutdown Talos across the whole cluster
    prompt: Shutdown the Talos cluster ... continue?
    cmd: talosctl --nodes {{.NODES}} shutdown --force
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.NODES}} get machineconfig
      - which jq talosctl

  reset-node:
    desc: Reset Talos on a single node [NODE=required]
    prompt: Reset Talos node '{{.NODE}}' ... continue?
    cmd: talosctl --nodes {{.NODE}} reset --graceful=false
    requires:
      vars: [NODE]
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.NODE}} get machineconfig
      - which talosctl

  reset-node-maintenance:
    desc: Reset Talos node to maintenance mode [NODE=required]
    prompt: Reset Talos node '{{.NODE}}' to maintenance mode ... continue?
    cmd: talosctl --nodes {{.NODE}} reset --graceful=false --reboot=false
    requires:
      vars: [NODE]
    preconditions:
      - talosctl config info
      - which talosctl

  reset-cluster:
    desc: Reset Talos across the whole cluster
    prompt: Reset the Talos cluster ... continue?
    cmd: talosctl --nodes {{.NODES}} reset --graceful=false
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.NODES}} get machineconfig
      - which jq talosctl

  reset-cluster-maintenance:
    desc: Reset Talos cluster and leave nodes in maintenance mode
    prompt: Reset the Talos cluster to maintenance mode ... continue?
    cmd: talosctl --nodes {{.NODES}} reset --graceful=false --reboot=false
    vars:
      NODES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    preconditions:
      - talosctl config info
      - which jq talosctl

  reset-cluster-maintenance-force:
    desc: Reset Talos cluster to maintenance mode (bypass connectivity checks)
    prompt: Force reset the Talos cluster to maintenance mode ... continue?
    cmd: talosctl --nodes {{.TALOS_NODES}} reset --graceful=false --reboot=false
    preconditions:
      - which talosctl

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster
    cmd: talosctl kubeconfig --nodes {{.RANDOM_CONTROLLER}} --force {{.KUBERNETES_DIR}}
    vars:
      RANDOM_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | sort -R | head -1
    preconditions:
      - talosctl config info
      - talosctl --nodes {{.RANDOM_CONTROLLER}} get machineconfig
      - which jq talosctl

  backup-secrets:
    desc: Backup current secrets with timestamp
    cmds:
      - |
        BACKUP_DIR="{{.ROOT_DIR}}/secrets-backups"
        TIMESTAMP=$(date +%Y%m%d-%H%M%S)
        mkdir -p "$BACKUP_DIR"
        
        [ -f {{.ROOT_DIR}}/secrets.yaml ] && cp {{.ROOT_DIR}}/secrets.yaml "$BACKUP_DIR/secrets-$TIMESTAMP.yaml" && echo "Backed up secrets.yaml"
        [ -f {{.TALOS_DIR}}/talosconfig ] && cp {{.TALOS_DIR}}/talosconfig "$BACKUP_DIR/talosconfig-$TIMESTAMP" && echo "Backed up talosconfig"

  list-backups:
    desc: List available backups
    cmd: ls -1 {{.ROOT_DIR}}/secrets-backups/secrets-*.yaml 2>/dev/null | sed 's|.*/secrets-||; s|\.yaml||' | sort -r || echo "No backups found"

  restore-secrets:
    desc: Restore from backup [BACKUP=timestamp]
    requires:
      vars: [BACKUP]
    cmds:
      - task: backup-secrets
      - cp {{.ROOT_DIR}}/secrets-backups/secrets-{{.BACKUP}}.yaml {{.ROOT_DIR}}/secrets.yaml
      - cp {{.ROOT_DIR}}/secrets-backups/talosconfig-{{.BACKUP}} {{.TALOS_DIR}}/talosconfig 2>/dev/null || true
      - echo "Restored from backup {{.BACKUP}}"

  pull-secrets:
    desc: Pull secrets from 1Password (what nodes are using)
    cmds:
      - task: backup-secrets
      - task: build-secrets
      - task: generate-config
      - echo "Pulled secrets from 1Password"
    preconditions:
      - op user get --me
      - which op

  push-secrets:  
    desc: Push local secrets to 1Password (updates what nodes will use)
    cmds:
      - task: update-secrets
        vars:
          OP_VAULT: "{{.OP_VAULT}}"
      - echo "Pushed secrets to 1Password"
    preconditions: 
      - sh: test -f {{.ROOT_DIR}}/secrets.yaml
        msg: "No secrets.yaml found"
      - op user get --me

  gen-secrets:
    desc: Generate new secrets and push to 1Password [FORCE=true to skip prompt]
    cmds:
      - task: backup-secrets
      - |
        if [ -f {{.ROOT_DIR}}/secrets.yaml ] && [ "{{.FORCE}}" != "true" ]; then
            printf "Overwrite existing secrets? (y/n) "
            read -r reply
            [[ "$reply" =~ ^[Yy] ]] || exit 1
        fi
        talosctl gen secrets --force
      - task: push-secrets
    preconditions:
      - op user get --me
      - which op
      - which talosctl
      - which yq

  update-secrets:
    desc: Update 1Password with generated cluster secrets
    cmd: |-
      if [ ! -f {{.ROOT_DIR}}/secrets.yaml ]; then
        echo "Error: secrets.yaml not found. Run 'task talos:gen-secrets' or 'task talos:build-secrets' first."
        exit 1
      fi

      echo "Updating 1Password with cluster secrets..."

      # Extract values from secrets.yaml
      CLUSTER_ID=$(yq '.cluster.id' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_SECRET=$(yq '.cluster.secret' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_TOKEN=$(yq '.secrets.bootstraptoken' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_SECRETBOXENCRYPTIONSECRET=$(yq '.secrets.secretboxencryptionsecret' {{.ROOT_DIR}}/secrets.yaml)

      # Machine certificates (OS certificates are used for machine API access)
      MACHINE_TOKEN=$(yq '.trustdinfo.token' {{.ROOT_DIR}}/secrets.yaml)
      MACHINE_CA_CRT=$(yq '.certs.os.crt' {{.ROOT_DIR}}/secrets.yaml)
      MACHINE_CA_KEY=$(yq '.certs.os.key' {{.ROOT_DIR}}/secrets.yaml)

      CLUSTER_CA_CRT=$(yq '.certs.k8s.crt' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_CA_KEY=$(yq '.certs.k8s.key' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_AGGREGATORCA_CRT=$(yq '.certs.k8saggregator.crt' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_AGGREGATORCA_KEY=$(yq '.certs.k8saggregator.key' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_ETCD_CA_CRT=$(yq '.certs.etcd.crt' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_ETCD_CA_KEY=$(yq '.certs.etcd.key' {{.ROOT_DIR}}/secrets.yaml)
      CLUSTER_SERVICEACCOUNT_KEY=$(yq '.certs.k8sserviceaccount.key' {{.ROOT_DIR}}/secrets.yaml)

      # Update 1Password with all secrets
      op item edit "talos" --vault "{{.OP_VAULT}}" \
        "CLUSTER_ID[text]=$CLUSTER_ID" \
        "CLUSTER_SECRET[text]=$CLUSTER_SECRET" \
        "CLUSTER_TOKEN[text]=$CLUSTER_TOKEN" \
        "CLUSTER_SECRETBOXENCRYPTIONSECRET[text]=$CLUSTER_SECRETBOXENCRYPTIONSECRET" \
        "MACHINE_TOKEN[text]=$MACHINE_TOKEN" \
        "MACHINE_CA_CRT[text]=$MACHINE_CA_CRT" \
        "MACHINE_CA_KEY[text]=$MACHINE_CA_KEY" \
        "CLUSTER_CA_CRT[text]=$CLUSTER_CA_CRT" \
        "CLUSTER_CA_KEY[text]=$CLUSTER_CA_KEY" \
        "CLUSTER_AGGREGATORCA_CRT[text]=$CLUSTER_AGGREGATORCA_CRT" \
        "CLUSTER_AGGREGATORCA_KEY[text]=$CLUSTER_AGGREGATORCA_KEY" \
        "CLUSTER_ETCD_CA_CRT[text]=$CLUSTER_ETCD_CA_CRT" \
        "CLUSTER_ETCD_CA_KEY[text]=$CLUSTER_ETCD_CA_KEY" \
        "CLUSTER_SERVICEACCOUNT_KEY[text]=$CLUSTER_SERVICEACCOUNT_KEY"

      echo "Successfully updated 1Password with all cluster secrets"

      # Update external secrets credentials to be properly formatted as a JSON string
      echo "Updating 1Password external secrets credentials..."
      if op item get 1password --field OP_CREDENTIALS_JSON --vault homelab --reveal >/dev/null 2>&1; then
        OP_CREDENTIALS_JSON=$(op item get 1password --field OP_CREDENTIALS_JSON --vault homelab --reveal)
        # Only process if we have valid JSON
        if echo "$OP_CREDENTIALS_JSON" | jq -e . >/dev/null 2>&1; then
          # Escape the JSON properly as a string
          OP_CREDENTIALS_ESCAPED=$(echo "$OP_CREDENTIALS_JSON" | jq -c . | jq -R .)
          op item edit 1password "OP_CREDENTIALS_JSON=$OP_CREDENTIALS_ESCAPED" --vault homelab
          echo "Successfully updated 1Password external secrets credentials"
        else
          echo "Warning: OP_CREDENTIALS_JSON field contains a string, not a JSON object. Assuming it's already escaped."
        fi
      else
        echo "Warning: OP_CREDENTIALS_JSON field not found in 1password item, skipping update"
      fi
    preconditions:
      - op user get --me
      - sh: test -f {{.ROOT_DIR}}/secrets.yaml
        msg: "secrets.yaml not found. Run 'task talos:build-secrets' to build it from 1Password, or 'task talos:gen-secrets' to generate new ones"
      - which op yq jq

  build-secrets:
    desc: Build secrets.yaml from 1Password credentials
    cmd: |-
      echo "Building secrets.yaml from 1Password..."

      # Get all secrets from 1Password
      CLUSTER_ID=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_ID")
      CLUSTER_SECRET=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_SECRET")
      CLUSTER_TOKEN=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_TOKEN")
      CLUSTER_SECRETBOXENCRYPTIONSECRET=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_SECRETBOXENCRYPTIONSECRET")
      MACHINE_TOKEN=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "MACHINE_TOKEN")
      MACHINE_CA_CRT=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "MACHINE_CA_CRT")
      MACHINE_CA_KEY=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "MACHINE_CA_KEY")
      CLUSTER_CA_CRT=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_CA_CRT")
      CLUSTER_CA_KEY=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_CA_KEY")
      CLUSTER_AGGREGATORCA_CRT=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_AGGREGATORCA_CRT")
      CLUSTER_AGGREGATORCA_KEY=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_AGGREGATORCA_KEY")
      CLUSTER_ETCD_CA_CRT=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_ETCD_CA_CRT")
      CLUSTER_ETCD_CA_KEY=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_ETCD_CA_KEY")
      CLUSTER_SERVICEACCOUNT_KEY=$(op item get "talos" --vault "{{.OP_VAULT}}" --field "CLUSTER_SERVICEACCOUNT_KEY")

      # Build the secrets.yaml file
      cat > {{.ROOT_DIR}}/secrets.yaml << EOF
      cluster:
        id: $CLUSTER_ID
        secret: $CLUSTER_SECRET
      secrets:
        bootstraptoken: $CLUSTER_TOKEN
        secretboxencryptionsecret: $CLUSTER_SECRETBOXENCRYPTIONSECRET
      trustdinfo:
        token: $MACHINE_TOKEN
      certs:
        etcd:
          crt: $CLUSTER_ETCD_CA_CRT
          key: $CLUSTER_ETCD_CA_KEY
        k8s:
          crt: $CLUSTER_CA_CRT
          key: $CLUSTER_CA_KEY
        k8saggregator:
          crt: $CLUSTER_AGGREGATORCA_CRT
          key: $CLUSTER_AGGREGATORCA_KEY
        k8sserviceaccount:
          key: $CLUSTER_SERVICEACCOUNT_KEY
        os:
          crt: $MACHINE_CA_CRT
          key: $MACHINE_CA_KEY
      EOF

      echo "Successfully built secrets.yaml from 1Password credentials"
    generates:
      - "{{.ROOT_DIR}}/secrets.yaml"
    preconditions:
      - op user get --me
      - which op

  remove-node:
    desc: Remove a node from cluster and update configs [NODE=required]
    requires:
      vars: [NODE]
    vars:
      NODE_IP:
        sh: |
          # If NODE is already an IP address, use it directly
          if echo "{{.NODE}}" | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$' >/dev/null; then
            echo "{{.NODE}}"
          else
            # Map hostname to IP address using node mapping file
            grep "{{.NODE}}" node-mapping.yaml | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+\.[0-9]\+' | head -1 || echo "{{.NODE}}"
          fi
    cmds:
      - echo "Removing node {{.NODE}} ({{.NODE_IP}}) from cluster..."
      # Drain the node in Kubernetes
      - kubectl drain {{.NODE}} --ignore-daemonsets --delete-emptydir-data --force --grace-period=0 || true
      # Delete the node from Kubernetes
      - kubectl delete node {{.NODE}} || true
      # Try to remove from etcd cluster (will fail if node is unreachable, which is fine)
      - |
        echo "Attempting to remove from etcd cluster..."
        if talosctl --nodes {{.NODE_IP}} etcd leave >/dev/null 2>&1; then
          echo "Node successfully left etcd cluster"
        else
          echo "Node could not leave etcd cluster (node may be unreachable)"
          echo "Attempting to remove etcd member forcefully..."
          # Get member ID and remove it
          MEMBER_ID=$(talosctl etcd members | grep "{{.NODE_IP}}" | awk '{print $2}' || echo "")
          if [ -n "$MEMBER_ID" ]; then
            talosctl etcd remove-member "$MEMBER_ID" || echo "Failed to remove etcd member"
          else
            echo "No etcd member found for {{.NODE_IP}}"
          fi
        fi
      # Remove node from node-mapping.yaml to update source of truth
      - |
        echo "Removing {{.NODE_IP}} from node-mapping.yaml..."
        yq eval 'del(.nodes["{{.NODE_IP}}"])' -i node-mapping.yaml
      # Remove node references from other node configs (certSANs) but keep the node's own config file
      - |
        echo "Removing {{.NODE}} references from other node static configs..."
        for config in static-configs/*.yaml; do
          filename=$(basename "$config" .yaml)
          # Skip the node's own config file - we might want to put it back in rotation
          if [ "$filename" = "{{.NODE}}" ]; then
            echo "Keeping {{.NODE}}.yaml config file for potential future use"
            continue
          fi
          
          echo "Updating $config..."
          # Remove the FQDN and hostname entries for the node
          sed -i '/- "{{.NODE}}.hypyr.space"/d' "$config" 2>/dev/null || true
          sed -i '/- "{{.NODE}}"/d' "$config" 2>/dev/null || true
        done
      # Regenerate talosconfig with remaining healthy nodes
      - task: generate-config-healthy
      - echo "✅ Node {{.NODE}} removed from cluster and static configs updated"
    preconditions:
      - which kubectl talosctl

  bootstrap:
    desc: Bootstrap Talos controlplane nodes. [NODE=required]
    requires:
      vars: [NODE]
    cmds:
      - talosctl --nodes {{.NODE}} bootstrap
    preconditions:
      - op user get --me
      - talosctl --nodes {{.NODE}} get machineconfig
      - which talosctl
